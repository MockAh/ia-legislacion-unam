# ui_app.py
# VERSIN 1.0 - Lista para Despliegue

import streamlit as st
import time

# Reutilizamos las funciones y la configuraci贸n de tu script RAG.
# 隆Aseg煤rate de que este archivo est茅 en la misma carpeta que rag_app_deepseek.py!
from rag_app_deepseek import inicializar_sistema, DEEPSEEK_MODEL_NAME

# --- 1. CONFIGURACIN DE LA PGINA Y CARGA DEL SISTEMA ---

st.set_page_config(
    page_title="Asistente Legislaci贸n UNAM",
    page_icon="",
    layout="wide"
)

st.title(" Asistente de Legislaci贸n UNAM (Facultad de Ciencias)")
st.caption(f"Potenciado por DeepSeek ({DEEPSEEK_MODEL_NAME}) y una base de datos local.")

# Usamos el cache de Streamlit para inicializar el sistema solo una vez.
# Esto es CRUCIAL para el rendimiento en la nube.
@st.cache_resource
def cargar_recursos():
    """Carga la base de datos y el cliente de API. Muestra un spinner durante la carga."""
    # El spinner solo se mostrar谩 la primera vez que un usuario cargue la app.
    with st.spinner("Iniciando sistema por primera vez... Cargando base de datos y modelos. Esto puede tardar unos segundos."):
        db, client = inicializar_sistema()
    return db, client

db_instance, client_instance = cargar_recursos()

# Manejo de error si el sistema no pudo inicializar (p. ej., falta el 铆ndice FAISS)
if not db_instance or not client_instance:
    st.error("El sistema no pudo inicializarse. Verifica que el 铆ndice FAISS est茅 en el repositorio y que los 'Secrets' de la API est茅n configurados en Streamlit Cloud.", icon="")
    st.stop() # Detiene la ejecuci贸n de la app si hay un error cr铆tico.


# --- 2. LGICA DE GENERACIN DE RESPUESTA PARA LA UI ---

def generar_respuesta_stream(query, db, client):
    """
    Esta funci贸n adapta tu l贸gica de RAG para que funcione con st.write_stream.
    En lugar de imprimir, 'yield' entrega cada fragmento de la respuesta.
    """
    # 1. B煤squeda de similitud (igual que en tu script)
    retrieved_docs = db.similarity_search(query, k=5) 
    
    if not retrieved_docs:
        yield "La informaci贸n solicitada no se encuentra en los documentos disponibles."
        return

    # 2. Construcci贸n del contexto y fuentes (igual que en tu script)
    contexto = ""
    fuentes = set()
    for doc in retrieved_docs:
        contexto += f"--- Fragmento de: {doc.metadata.get('source', 'N/A')} ---\n"
        contexto += f"{doc.page_content}\n\n"
        fuentes.add(doc.metadata.get('source', 'N/A'))
    
    fuentes_str = "\n".join(f"- {f}" for f in fuentes)

    # 3. Construcci贸n del prompt (igual que en tu script)
    prompt_template = f"""
    Eres un asistente de IA especializado en la legislaci贸n y normatividad de la UNAM. Act煤a con m谩xima precisi贸n.
    INSTRUCCIONES:
    1. Tu 煤nica fuente de verdad es el CONTEXTO proporcionado. NO uses conocimiento externo.
    2. Responde a la PREGUNTA DEL USUARIO bas谩ndote exclusivamente en el CONTEXTO.
    3. Si la respuesta no est谩 en el CONTEXTO, responde: "La informaci贸n solicitada no se encuentra en los documentos disponibles."
    4. NO cites las fuentes en el cuerpo de la respuesta. La UI las mostrar谩 por separado.
    --- CONTEXTO ---
    {contexto}
    --- FIN DEL CONTEXTO ---
    PREGUNTA DEL USUARIO: "{query}"
    Respuesta:
    """

    # 4. Llamada a la API y 'yield' de los fragmentos
    try:
        stream_response = client.chat.completions.create(
            model=DEEPSEEK_MODEL_NAME,
            messages=[
                {"role": "system", "content": "Sigue las instrucciones del prompt del usuario."},
                {"role": "user", "content": prompt_template}
            ],
            stream=True,
            max_tokens=1500,
            temperature=0.1
        )
        for chunk in stream_response:
            content = chunk.choices[0].delta.content
            if content:
                yield content
                
    except Exception as e:
        yield f"Ocurri贸 un error al contactar la API de DeepSeek: {e}"

    # Al final, guardamos las fuentes en el estado de la sesi贸n para mostrarlas fuera del stream.
    st.session_state.fuentes = fuentes_str

# --- 3. INTERFAZ DE USUARIO ---

# Inicializar el historial de chat en el estado de la sesi贸n
if "messages" not in st.session_state:
    st.session_state.messages = []
if "fuentes" not in st.session_state:
    st.session_state.fuentes = ""

# Mostrar mensajes previos
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Aceptar la entrada del usuario
if prompt := st.chat_input("驴Qu茅 deseas saber sobre la legislaci贸n de la Facultad de Ciencias?"):
    # A帽adir el mensaje del usuario al historial y mostrarlo
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Generar y mostrar la respuesta del asistente
    with st.chat_message("assistant"):
        # Usamos st.write_stream para mostrar la respuesta en tiempo real
        response_placeholder = st.empty()
        full_response = response_placeholder.write_stream(generar_respuesta_stream(prompt, db_instance, client_instance))
        
        # Una vez que la respuesta est谩 completa, mostramos las fuentes si existen.
        if st.session_state.fuentes:
            with st.expander("Fuentes Consultadas"):
                st.markdown(st.session_state.fuentes)
            st.session_state.fuentes = "" # Limpiar para la siguiente pregunta

    # A帽adir la respuesta completa del asistente al historial
    st.session_state.messages.append({"role": "assistant", "content": full_response})